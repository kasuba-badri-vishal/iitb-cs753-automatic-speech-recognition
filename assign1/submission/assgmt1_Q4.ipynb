{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3S0Etz_Nokz"
   },
   "source": [
    "# PART I: Running a SpeechBrain ASR Recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-xktzpeEBjf"
   },
   "source": [
    "## You have to fill in appropriate code in 4 locations in the notebook.\n",
    "\n",
    "* The four locations start with \"####TASK\". Read the task specifications mentioned there.\n",
    "* The required function names are already available in this notebook somewhere. You're only required to find and use the appropriate ones.\n",
    "* Check the SpeechBrain documentation to find out how to use those functions. Refer to the starting material for more resources.\n",
    "* By the end of this part of the assignment, you should be comfortable running a Speechbrain recipe.\n",
    "* **P.S.** Note that none of the four tasks require you to write more than 1 line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKXIYTJSOnt_"
   },
   "source": [
    "### Setting up the codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A5H2lpHX7npZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'speechbrain'...\n",
      "remote: Enumerating objects: 86452, done.\u001b[K\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
      "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
      "remote: Total 86452 (delta 13), reused 20 (delta 7), pack-reused 86425\u001b[K\n",
      "Receiving objects: 100% (86452/86452), 81.54 MiB | 5.99 MiB/s, done.\n",
      "Resolving deltas: 100% (59244/59244), done.\n",
      "/raid/nlp/pranavg/meet/ASR/Assignment-1/speechbrain\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cusparse-cu12 (/raid/nlp/pranavg/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mIgnoring SoundFile: markers 'sys_platform == \"win32\"' don't match your environment\n",
      "Requirement already satisfied: black==19.10b0 in /raid/nlp/pranavg/anaconda3/envs/nmt/lib/python3.8/site-packages (from -r lint-requirements.txt (line 1)) (19.10b0)\n",
      "Requirement already satisfied: click==8.0.4 in /raid/nlp/pranavg/anaconda3/envs/nmt/lib/python3.8/site-packages (from -r lint-requirements.txt (line 2)) (8.0.4)\n",
      "Requirement already satisfied: flake8==3.7.9 in /raid/nlp/pranavg/anaconda3/envs/nmt/lib/python3.8/site-packages (from -r lint-requirements.txt (line 3)) (3.7.9)\n",
      "Requirement already satisfied: pycodestyle==2.5.0 in /raid/nlp/pranavg/anaconda3/envs/nmt/lib/python3.8/site-packages (from -r lint-requirements.txt (line 4)) (2.5.0)\n",
      "Requirement already satisfied: pytest==7.4.0 in /raid/nlp/pranavg/anaconda3/envs/nmt/lib/python3.8/site-packages (from -r lint-requirements.txt (line 5)) (7.4.0)\n",
      "Requirement already satisfied: yamllint==1.23.0 in /raid/nlp/pranavg/anaconda3/envs/nmt/lib/python3.8/site-packages (from -r lint-requirements.txt (line 6)) (1.23.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.8.0 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (0.20.3)\n",
      "Requirement already satisfied: hyperpyyaml>=0.0.1 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.2.2)\n",
      "Requirement already satisfied: joblib>=0.14.1 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.24.3)\n",
      "Requirement already satisfied: packaging in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (23.2)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (2.0.3)\n",
      "Requirement already satisfied: pre-commit>=2.3.0 in /raid/nlp/pranavg/anaconda3/envs/nmt/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (1.5.4)\n",
      "Requirement already satisfied: sentencepiece>=0.1.91 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (0.1.98)\n",
      "Requirement already satisfied: torch>=1.9.0 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from -r requirements.txt (line 12)) (2.1.2)\n",
      "Requirement already satisfied: torchaudio>=1.9.0 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (2.1.2)\n",
      "Requirement already satisfied: tqdm>=4.42.0 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from -r requirements.txt (line 14)) (4.66.1)\n",
      "Requirement already satisfied: transformers>=4.30.0 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from -r requirements.txt (line 15)) (4.37.1)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from black==19.10b0->-r lint-requirements.txt (line 1)) (23.2.0)\n",
      "Requirement already satisfied: appdirs in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from black==19.10b0->-r lint-requirements.txt (line 1)) (1.4.4)\n",
      "Requirement already satisfied: toml>=0.9.4 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from black==19.10b0->-r lint-requirements.txt (line 1)) (0.10.2)\n",
      "Requirement already satisfied: typed-ast>=1.4.0 in /raid/nlp/pranavg/anaconda3/envs/nmt/lib/python3.8/site-packages (from black==19.10b0->-r lint-requirements.txt (line 1)) (1.5.5)\n",
      "Requirement already satisfied: regex in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from black==19.10b0->-r lint-requirements.txt (line 1)) (2023.12.25)\n",
      "Requirement already satisfied: pathspec<1,>=0.6 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from black==19.10b0->-r lint-requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: entrypoints<0.4.0,>=0.3.0 in /raid/nlp/pranavg/anaconda3/envs/nmt/lib/python3.8/site-packages (from flake8==3.7.9->-r lint-requirements.txt (line 3)) (0.3)\n",
      "Requirement already satisfied: pyflakes<2.2.0,>=2.1.0 in /raid/nlp/pranavg/anaconda3/envs/nmt/lib/python3.8/site-packages (from flake8==3.7.9->-r lint-requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /raid/nlp/pranavg/anaconda3/envs/nmt/lib/python3.8/site-packages (from flake8==3.7.9->-r lint-requirements.txt (line 3)) (0.6.1)\n",
      "Requirement already satisfied: iniconfig in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from pytest==7.4.0->-r lint-requirements.txt (line 5)) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from pytest==7.4.0->-r lint-requirements.txt (line 5)) (0.13.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from pytest==7.4.0->-r lint-requirements.txt (line 5)) (1.1.3)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from pytest==7.4.0->-r lint-requirements.txt (line 5)) (1.2.3)\n",
      "Requirement already satisfied: pyyaml in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from yamllint==1.23.0->-r lint-requirements.txt (line 6)) (6.0.1)\n",
      "Requirement already satisfied: filelock in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from huggingface_hub>=0.8.0->-r requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from huggingface_hub>=0.8.0->-r requirements.txt (line 2)) (2023.10.0)\n",
      "Requirement already satisfied: requests in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from huggingface_hub>=0.8.0->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from huggingface_hub>=0.8.0->-r requirements.txt (line 2)) (4.5.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.28 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from hyperpyyaml>=0.0.1->-r requirements.txt (line 3)) (0.18.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from pandas>=1.0.1->-r requirements.txt (line 7)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from pandas>=1.0.1->-r requirements.txt (line 7)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from pandas>=1.0.1->-r requirements.txt (line 7)) (2023.4)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /raid/nlp/pranavg/anaconda3/envs/nmt/lib/python3.8/site-packages (from pre-commit>=2.3.0->-r requirements.txt (line 8)) (3.4.0)\n",
      "Requirement already satisfied: identify>=1.0.0 in /raid/nlp/pranavg/anaconda3/envs/nmt/lib/python3.8/site-packages (from pre-commit>=2.3.0->-r requirements.txt (line 8)) (2.5.34)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /raid/nlp/pranavg/anaconda3/envs/nmt/lib/python3.8/site-packages (from pre-commit>=2.3.0->-r requirements.txt (line 8)) (1.8.0)\n",
      "Requirement already satisfied: virtualenv>=20.10.0 in /raid/nlp/pranavg/anaconda3/envs/nmt/lib/python3.8/site-packages (from pre-commit>=2.3.0->-r requirements.txt (line 8)) (20.25.0)\n",
      "Requirement already satisfied: sympy in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9.0->-r requirements.txt (line 12)) (1.12)\n",
      "Requirement already satisfied: networkx in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9.0->-r requirements.txt (line 12)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9.0->-r requirements.txt (line 12)) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9.0->-r requirements.txt (line 12)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9.0->-r requirements.txt (line 12)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9.0->-r requirements.txt (line 12)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9.0->-r requirements.txt (line 12)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9.0->-r requirements.txt (line 12)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9.0->-r requirements.txt (line 12)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9.0->-r requirements.txt (line 12)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9.0->-r requirements.txt (line 12)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9.0->-r requirements.txt (line 12)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9.0->-r requirements.txt (line 12)) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9.0->-r requirements.txt (line 12)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9.0->-r requirements.txt (line 12)) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9.0->-r requirements.txt (line 12)) (12.3.101)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from transformers>=4.30.0->-r requirements.txt (line 15)) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from transformers>=4.30.0->-r requirements.txt (line 15)) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from nodeenv>=0.11.1->pre-commit>=2.3.0->-r requirements.txt (line 8)) (69.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from ruamel.yaml>=0.17.28->hyperpyyaml>=0.0.1->-r requirements.txt (line 3)) (0.2.8)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in /raid/nlp/pranavg/anaconda3/envs/nmt/lib/python3.8/site-packages (from virtualenv>=20.10.0->pre-commit>=2.3.0->-r requirements.txt (line 8)) (0.3.8)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from virtualenv>=20.10.0->pre-commit>=2.3.0->-r requirements.txt (line 8)) (3.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from jinja2->torch>=1.9.0->-r requirements.txt (line 12)) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from requests->huggingface_hub>=0.8.0->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from requests->huggingface_hub>=0.8.0->-r requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from requests->huggingface_hub>=0.8.0->-r requirements.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from requests->huggingface_hub>=0.8.0->-r requirements.txt (line 2)) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from sympy->torch>=1.9.0->-r requirements.txt (line 12)) (1.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cusparse-cu12 (/raid/nlp/pranavg/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -vidia-cusparse-cu12 (/raid/nlp/pranavg/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mObtaining file:///raid/nlp/pranavg/meet/ASR/Assignment-1/speechbrain\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[33m  WARNING: Ignoring invalid distribution -vidia-cusparse-cu12 (/raid/nlp/pranavg/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[?25hRequirement already satisfied: hyperpyyaml in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from speechbrain==0.5.16) (1.2.2)\n",
      "Requirement already satisfied: joblib in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from speechbrain==0.5.16) (1.3.2)\n",
      "Requirement already satisfied: numpy in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from speechbrain==0.5.16) (1.24.3)\n",
      "Requirement already satisfied: packaging in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from speechbrain==0.5.16) (23.2)\n",
      "Requirement already satisfied: scipy in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from speechbrain==0.5.16) (1.5.4)\n",
      "Requirement already satisfied: sentencepiece in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from speechbrain==0.5.16) (0.1.98)\n",
      "Requirement already satisfied: torch>=1.9 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from speechbrain==0.5.16) (2.1.2)\n",
      "Requirement already satisfied: torchaudio in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from speechbrain==0.5.16) (2.1.2)\n",
      "Requirement already satisfied: tqdm in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from speechbrain==0.5.16) (4.66.1)\n",
      "Requirement already satisfied: huggingface-hub in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from speechbrain==0.5.16) (0.20.3)\n",
      "Requirement already satisfied: filelock in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9->speechbrain==0.5.16) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9->speechbrain==0.5.16) (4.5.0)\n",
      "Requirement already satisfied: sympy in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9->speechbrain==0.5.16) (1.12)\n",
      "Requirement already satisfied: networkx in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9->speechbrain==0.5.16) (3.1)\n",
      "Requirement already satisfied: jinja2 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9->speechbrain==0.5.16) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9->speechbrain==0.5.16) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9->speechbrain==0.5.16) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9->speechbrain==0.5.16) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9->speechbrain==0.5.16) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9->speechbrain==0.5.16) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9->speechbrain==0.5.16) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9->speechbrain==0.5.16) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9->speechbrain==0.5.16) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9->speechbrain==0.5.16) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9->speechbrain==0.5.16) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9->speechbrain==0.5.16) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9->speechbrain==0.5.16) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from torch>=1.9->speechbrain==0.5.16) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9->speechbrain==0.5.16) (12.3.101)\n",
      "Requirement already satisfied: requests in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from huggingface-hub->speechbrain==0.5.16) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from huggingface-hub->speechbrain==0.5.16) (6.0.1)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.28 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from hyperpyyaml->speechbrain==0.5.16) (0.18.6)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain==0.5.16) (0.2.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from jinja2->torch>=1.9->speechbrain==0.5.16) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from requests->huggingface-hub->speechbrain==0.5.16) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from requests->huggingface-hub->speechbrain==0.5.16) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from requests->huggingface-hub->speechbrain==0.5.16) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from requests->huggingface-hub->speechbrain==0.5.16) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /raid/nlp/pranavg/.local/lib/python3.8/site-packages (from sympy->torch>=1.9->speechbrain==0.5.16) (1.3.0)\n",
      "Building wheels for collected packages: speechbrain\n",
      "  Building editable for speechbrain (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for speechbrain: filename=speechbrain-0.5.16-0.editable-py3-none-any.whl size=14844 sha256=d451a6bb7a599400e9140092e8fa3db031462f89a539f134054cdfcb4d9cb2c6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-d9skybbi/wheels/b2/9f/f6/a5d0607b3d7c006cbdab1c553cb2e198b1db216ba26e484e80\n",
      "Successfully built speechbrain\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -vidia-cusparse-cu12 (/raid/nlp/pranavg/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: speechbrain\n",
      "  Attempting uninstall: speechbrain\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -vidia-cusparse-cu12 (/raid/nlp/pranavg/.local/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: speechbrain 0.5.16\n",
      "    Uninstalling speechbrain-0.5.16:\n",
      "      Successfully uninstalled speechbrain-0.5.16\n",
      "Successfully installed speechbrain-0.5.16\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "!rm -rf speechbrain/\n",
    "# Clone SpeechBrain repository\n",
    "!git clone https://github.com/Darshan7575/speechbrain.git\n",
    "%cd ./speechbrain/\n",
    "\n",
    "# Install required dependencies\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Install SpeechBrain in editable mode\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid/nlp/pranavg/meet/ASR/Assignment-1/speechbrain\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "61C_gxJo5aRH"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# Required imports\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import speechbrain as sb\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "from speechbrain.utils.data_utils import get_all_files, download_file\n",
    "from speechbrain.dataio.dataio import read_audio\n",
    "from speechbrain.utils.distributed import run_on_main, if_main_process\n",
    "\n",
    "# Required variables and loggers\n",
    "logger = logging.getLogger(__name__)\n",
    "logger = logging.getLogger(__name__)\n",
    "MINILIBRI_TRAIN_URL = \"http://www.openslr.org/resources/31/train-clean-5.tar.gz\"\n",
    "MINILIBRI_VALID_URL = \"http://www.openslr.org/resources/31/dev-clean-2.tar.gz\"\n",
    "MINILIBRI_TEST_URL = \"https://www.openslr.org/resources/12/test-clean.tar.gz\"\n",
    "SAMPLERATE = 16000\n",
    "\n",
    "device=\"cuda\"\n",
    "run_opts = {'device':device}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtdr1VnyCQTJ"
   },
   "source": [
    "### Tokenizer Training\n",
    "In this section, we will train a BPE tokenizer with **150 tokens** using `Sentencepiece`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "ujToJHWC4T5y"
   },
   "outputs": [],
   "source": [
    "# ############################################################################\n",
    "# Dataset creation helper functions\n",
    "# ############################################################################\n",
    "\n",
    "def prepare_mini_librispeech(\n",
    "    data_folder, save_json_train, save_json_valid, save_json_test\n",
    "):\n",
    "    \"\"\"\n",
    "    Prepares the json files for the Mini Librispeech dataset.\n",
    "    Downloads the dataset if its not found in the `data_folder`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if this phase is already done (if so, skip it)\n",
    "    if skip(save_json_train, save_json_valid, save_json_test):\n",
    "        logger.info(\"Preparation completed in previous run, skipping.\")\n",
    "        return\n",
    "\n",
    "    # If the dataset doesn't exist yet, download it\n",
    "    train_folder = os.path.join(data_folder, \"LibriSpeech\", \"train-clean-5\")\n",
    "    valid_folder = os.path.join(data_folder, \"LibriSpeech\", \"dev-clean-2\")\n",
    "    test_folder = os.path.join(data_folder, \"LibriSpeech\", \"test-clean\")\n",
    "    if not check_folders(train_folder, valid_folder, test_folder):\n",
    "        download_mini_librispeech(data_folder)\n",
    "\n",
    "    # List files and create manifest from list\n",
    "    logger.info(\n",
    "        f\"Creating {save_json_train}, {save_json_valid}, and {save_json_test}\"\n",
    "    )\n",
    "    extension = [\".flac\"]\n",
    "\n",
    "    # List of flac audio files\n",
    "    wav_list_train = get_all_files(train_folder, match_and=extension)\n",
    "    wav_list_valid = get_all_files(valid_folder, match_and=extension)\n",
    "    wav_list_test = get_all_files(test_folder, match_and=extension)\n",
    "\n",
    "    # List of transcription file\n",
    "    extension = [\".trans.txt\"]\n",
    "    trans_list = get_all_files(data_folder, match_and=extension)\n",
    "    trans_dict = get_transcription(trans_list)\n",
    "\n",
    "    # Create the json files\n",
    "    create_json(wav_list_train, trans_dict, save_json_train)\n",
    "    create_json(wav_list_valid, trans_dict, save_json_valid)\n",
    "    create_json(wav_list_test, trans_dict, save_json_test)\n",
    "\n",
    "\n",
    "def get_transcription(trans_list):\n",
    "    \"\"\"\n",
    "    Returns a dictionary with the transcription of each sentence in the dataset.\n",
    "    \"\"\"\n",
    "    # Processing all the transcription files in the list\n",
    "    trans_dict = {}\n",
    "    for trans_file in trans_list:\n",
    "        # Reading the text file\n",
    "        with open(trans_file) as f:\n",
    "            for line in f:\n",
    "                uttid = line.split(\" \")[0]\n",
    "                text = line.rstrip().split(\" \")[1:]\n",
    "                text = \" \".join(text)\n",
    "                trans_dict[uttid] = text\n",
    "\n",
    "    logger.info(\"Transcription files read!\")\n",
    "    return trans_dict\n",
    "\n",
    "\n",
    "def create_json(wav_list, trans_dict, json_file):\n",
    "    \"\"\"\n",
    "    Creates the json file given a list of wav files and their transcriptions.\n",
    "    \"\"\"\n",
    "    # Processing all the wav files in the list\n",
    "    json_dict = {}\n",
    "    for wav_file in wav_list:\n",
    "\n",
    "        # Reading the signal (to retrieve duration in seconds)\n",
    "        signal = read_audio(wav_file)\n",
    "        duration = signal.shape[0] / SAMPLERATE\n",
    "\n",
    "        # Manipulate path to get relative path and uttid\n",
    "        path_parts = wav_file.split(os.path.sep)\n",
    "        uttid, _ = os.path.splitext(path_parts[-1])\n",
    "        relative_path = os.path.join(\"{data_root}\", *path_parts[-5:])\n",
    "\n",
    "        # Create entry for this utterance\n",
    "        json_dict[uttid] = {\n",
    "            \"wav\": relative_path,\n",
    "            \"length\": duration,\n",
    "            \"words\": trans_dict[uttid],\n",
    "        }\n",
    "\n",
    "    # Writing the dictionary to the json file\n",
    "    with open(json_file, mode=\"w\") as json_f:\n",
    "        json.dump(json_dict, json_f, indent=2)\n",
    "\n",
    "    logger.info(f\"{json_file} successfully created!\")\n",
    "\n",
    "\n",
    "def skip(*filenames):\n",
    "    \"\"\"\n",
    "    Detects if the data preparation has been already done.\n",
    "    If the preparation has been done, we can skip it.\n",
    "    \"\"\"\n",
    "    for filename in filenames:\n",
    "        if not os.path.isfile(filename):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def check_folders(*folders):\n",
    "    \"\"\"Returns False if any passed folder does not exist.\"\"\"\n",
    "    for folder in folders:\n",
    "        if not os.path.exists(folder):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def download_mini_librispeech(destination):\n",
    "    \"\"\"Download dataset and unpack it.\n",
    "    \"\"\"\n",
    "    train_archive = os.path.join(destination, \"train-clean-5.tar.gz\")\n",
    "    valid_archive = os.path.join(destination, \"dev-clean-2.tar.gz\")\n",
    "    test_archive = os.path.join(destination, \"test-clean.tar.gz\")\n",
    "    download_file(MINILIBRI_TRAIN_URL, train_archive)\n",
    "    download_file(MINILIBRI_VALID_URL, valid_archive)\n",
    "    download_file(MINILIBRI_TEST_URL, test_archive)\n",
    "    shutil.unpack_archive(train_archive, destination)\n",
    "    shutil.unpack_archive(valid_archive, destination)\n",
    "    shutil.unpack_archive(test_archive, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "rz9pyHan4V0S"
   },
   "outputs": [],
   "source": [
    "tokenizer_hyperparams = \"\"\"\n",
    "# ############################################################################\n",
    "# Tokenizer: subword BPE with unigram 150\n",
    "# ############################################################################\n",
    "\n",
    "output_folder: !ref results/tokenizer/\n",
    "\n",
    "# Data files\n",
    "data_folder: data\n",
    "train_annotation: !ref <data_folder>/train.json\n",
    "valid_annotation: !ref <data_folder>/valid.json\n",
    "test_annotation: !ref <data_folder>/test.json\n",
    "\n",
    "# Tokenizer training parameters\n",
    "token_type: unigram  # [\"unigram\", \"bpe\", \"char\"]\n",
    "token_output: 150  # index(blank/eos/bos/unk) = 0\n",
    "character_coverage: 1.0\n",
    "json_read: words\n",
    "\n",
    "tokenizer: !name:speechbrain.tokenizers.SentencePiece.SentencePiece\n",
    "   model_dir: !ref <output_folder>\n",
    "   vocab_size: !ref <token_output>\n",
    "   annotation_train: !ref <train_annotation>\n",
    "   annotation_read: !ref <json_read>\n",
    "   annotation_format: json\n",
    "   model_type: !ref <token_type> # [\"unigram\", \"bpe\", \"char\"]\n",
    "   character_coverage: !ref <character_coverage>\n",
    "   annotation_list_to_check: [!ref <train_annotation>, !ref <valid_annotation>]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "nA8xD6Do4Xl7",
    "outputId": "27947756-ec98-4294-abe8-3a38bc078921"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: results/tokenizer/\n",
      "__main__ - Preparation completed in previous run, skipping.\n",
      "speechbrain.tokenizers.SentencePiece - Train tokenizer with type:unigram\n",
      "speechbrain.tokenizers.SentencePiece - Extract words sequences from:data/train.json\n",
      "speechbrain.tokenizers.SentencePiece - Text file created at: results/tokenizer/train.txt\n",
      "speechbrain.tokenizers.SentencePiece - ==== Loading Tokenizer ===\n",
      "speechbrain.tokenizers.SentencePiece - Tokenizer path: results/tokenizer/150_unigram.model\n",
      "speechbrain.tokenizers.SentencePiece - Tokenizer vocab_size: 150\n",
      "speechbrain.tokenizers.SentencePiece - Tokenizer type: unigram\n",
      "speechbrain.tokenizers.SentencePiece - ==== Accuracy checking for recovering text from tokenizer ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=results/tokenizer/train.txt --model_prefix=results/tokenizer/150_unigram --model_type=unigram --bos_id=-1 --eos_id=-1 --pad_id=-1 --unk_id=0 --max_sentencepiece_length=10 --character_coverage=1.0 --add_dummy_prefix=True --vocab_size=150\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: results/tokenizer/train.txt\n",
      "  input_format: \n",
      "  model_prefix: results/tokenizer/150_unigram\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 150\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 10\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: -1\n",
      "  eos_id: -1\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: results/tokenizer/train.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 1519 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=281212\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=28\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 1519 sentences.\n",
      "unigram_model_trainer.cc(247) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(251) LOG(INFO) Extracting frequent sub strings... node_num=143660\n",
      "unigram_model_trainer.cc(301) LOG(INFO) Initialized 21862 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 1519\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 7230\n",
      "unigram_model_trainer.cc(607) LOG(INFO) Using 7230 sentences for EM training\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=0 size=5525 obj=11.4707 num_tokens=14950 num_tokens/piece=2.70588\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=1 size=4978 obj=8.05265 num_tokens=13779 num_tokens/piece=2.76798\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=0 size=3732 obj=7.92615 num_tokens=15128 num_tokens/piece=4.05359\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=1 size=3727 obj=7.84039 num_tokens=15128 num_tokens/piece=4.05903\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=0 size=2795 obj=8.20349 num_tokens=17169 num_tokens/piece=6.14275\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=1 size=2794 obj=8.12796 num_tokens=17171 num_tokens/piece=6.14567\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=0 size=2095 obj=8.60462 num_tokens=19577 num_tokens/piece=9.34463\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=1 size=2095 obj=8.52756 num_tokens=19579 num_tokens/piece=9.34558\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=0 size=1571 obj=9.07475 num_tokens=21860 num_tokens/piece=13.9147\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=1 size=1571 obj=9.0096 num_tokens=21868 num_tokens/piece=13.9198\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=0 size=1178 obj=9.66684 num_tokens=24151 num_tokens/piece=20.5017\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=1 size=1178 obj=9.59504 num_tokens=24152 num_tokens/piece=20.5025\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=0 size=883 obj=10.3004 num_tokens=26360 num_tokens/piece=29.8528\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=1 size=883 obj=10.2183 num_tokens=26364 num_tokens/piece=29.8573\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=0 size=662 obj=11.0592 num_tokens=28801 num_tokens/piece=43.506\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=1 size=662 obj=10.9654 num_tokens=28813 num_tokens/piece=43.5242\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=0 size=496 obj=11.9396 num_tokens=31630 num_tokens/piece=63.7702\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=1 size=496 obj=11.8184 num_tokens=31649 num_tokens/piece=63.8085\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=0 size=372 obj=13.1149 num_tokens=33707 num_tokens/piece=90.6102\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=1 size=372 obj=13.0209 num_tokens=33757 num_tokens/piece=90.7446\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=0 size=279 obj=14.3279 num_tokens=35847 num_tokens/piece=128.484\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=1 size=279 obj=14.2495 num_tokens=35859 num_tokens/piece=128.527\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=0 size=209 obj=16.095 num_tokens=38013 num_tokens/piece=181.88\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=1 size=209 obj=16.0195 num_tokens=38020 num_tokens/piece=181.914\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=0 size=165 obj=18.2561 num_tokens=41004 num_tokens/piece=248.509\n",
      "unigram_model_trainer.cc(623) LOG(INFO) EM sub_iter=1 size=165 obj=18.1545 num_tokens=41050 num_tokens/piece=248.788\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: results/tokenizer/150_unigram.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: results/tokenizer/150_unigram.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.tokenizers.SentencePiece - recover words from: data/train.json\n",
      "speechbrain.tokenizers.SentencePiece - Wrong recover words: 0\n",
      "speechbrain.tokenizers.SentencePiece - accuracy recovering words: 1.0\n",
      "speechbrain.tokenizers.SentencePiece - ==== Accuracy checking for recovering text from tokenizer ===\n",
      "speechbrain.tokenizers.SentencePiece - recover words from: data/valid.json\n",
      "speechbrain.tokenizers.SentencePiece - Wrong recover words: 0\n",
      "speechbrain.tokenizers.SentencePiece - accuracy recovering words: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'results/tokenizer//tokenizer.ckpt'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load required params from the hyperpyyaml file\n",
    "hparams = load_hyperpyyaml(tokenizer_hyperparams)\n",
    "\n",
    "# 1. Dataset creation\n",
    "\n",
    "## Create experiment directory\n",
    "sb.create_experiment_directory(\n",
    "    experiment_directory=hparams[\"output_folder\"],\n",
    "    overrides=None,\n",
    ")\n",
    "\n",
    "## Create dataset\n",
    "run_on_main(\n",
    "    prepare_mini_librispeech,\n",
    "    kwargs={\n",
    "        \"data_folder\": hparams[\"data_folder\"],\n",
    "        \"save_json_train\": hparams[\"train_annotation\"],\n",
    "        \"save_json_valid\": hparams[\"valid_annotation\"],\n",
    "        \"save_json_test\": hparams[\"test_annotation\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2. Tokenizer training\n",
    "hparams[\"tokenizer\"]()\n",
    "\n",
    "# 3. Saving tokenizer in .ckpt extension\n",
    "output_path = hparams[\"output_folder\"]\n",
    "token_output = hparams[\"token_output\"]\n",
    "token_type = hparams[\"token_type\"]\n",
    "bpe_model = f\"{output_path}/{token_output}_{token_type}.model\"\n",
    "tokenizer_ckpt = f\"{output_path}/tokenizer.ckpt\"\n",
    "shutil.copyfile(bpe_model, tokenizer_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwqFx3QcOdd3"
   },
   "source": [
    "### Model Training\n",
    "In this section, we will train a **6 layer Conformer** encoder only architecture with the `CTC objective`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "6WPcwXueCRm7"
   },
   "outputs": [],
   "source": [
    "global_hyperparams = \"\"\"\n",
    "# Seed needs to be set at top of yaml, before objects with parameters are made\n",
    "seed: 2024\n",
    "__set_seed: !apply:torch.manual_seed [!ref <seed>]\n",
    "\n",
    "# Data files\n",
    "data_folder: data\n",
    "\n",
    "####TASK ADD APPROPRIATE REFERENCES TO LOAD THE FILES ##############\n",
    "\n",
    "train_annotation: !ref <data_folder>/train.json\n",
    "valid_annotation: !ref <data_folder>/valid.json\n",
    "test_annotation: !ref <data_folder>/test.json\n",
    "#####################################################################\n",
    "\n",
    "# Language model (LM) pretraining\n",
    "pretrained_lm_tokenizer_path: ./results/tokenizer\n",
    "\n",
    "# Training parameters\n",
    "number_of_epochs: 70\n",
    "batch_size: 8\n",
    "lr_adam: 0.01\n",
    "max_grad_norm: 5.0\n",
    "ckpt_interval_minutes: 15 # save checkpoint every N min\n",
    "loss_reduction: 'batchmean'\n",
    "\n",
    "# Dataloader options\n",
    "train_dataloader_opts:\n",
    "    batch_size: !ref <batch_size>\n",
    "\n",
    "valid_dataloader_opts:\n",
    "    batch_size: !ref <batch_size>\n",
    "\n",
    "test_dataloader_opts:\n",
    "    batch_size: !ref <batch_size>\n",
    "\n",
    "# Feature parameters\n",
    "sample_rate: 16000\n",
    "n_fft: 400\n",
    "n_mels: 40\n",
    "\n",
    "####################### Model parameters ###########################\n",
    "# Transformer\n",
    "d_model: 128 #64\n",
    "nhead: 4 #4\n",
    "num_encoder_layers: 6\n",
    "d_ffn: 512 #256\n",
    "transformer_dropout: 0.1\n",
    "activation: !name:torch.nn.GELU\n",
    "output_neurons: 150\n",
    "label_smoothing: 0.0\n",
    "attention_type: RelPosMHAXL\n",
    "\n",
    "# Outputs\n",
    "blank_index: 0\n",
    "pad_index: 0\n",
    "bos_index: 1\n",
    "eos_index: 2\n",
    "\n",
    "# Decoding parameters\n",
    "min_decode_ratio: 0.0\n",
    "max_decode_ratio: 1.0\n",
    "test_beam_size: 1\n",
    "ctc_weight_decode: 1.0\n",
    "\n",
    "############################## models ################################\n",
    "\n",
    "compute_features: !new:speechbrain.lobes.features.Fbank\n",
    "    sample_rate: !ref <sample_rate>\n",
    "    n_fft: !ref <n_fft>\n",
    "    n_mels: !ref <n_mels>\n",
    "\n",
    "CNN: !new:speechbrain.lobes.models.convolution.ConvolutionFrontEnd\n",
    "    input_shape: (8, 10, 40)\n",
    "    num_blocks: 2\n",
    "    num_layers_per_block: 1\n",
    "    out_channels: (64, 32)\n",
    "    kernel_sizes: (3, 3)\n",
    "    strides: (2, 2)\n",
    "    residuals: (False, False)\n",
    "\n",
    "# standard parameters for the BASE model\n",
    "Transformer: !new:speechbrain.lobes.models.transformer.TransformerASR.TransformerASR\n",
    "    input_size: 320 #640\n",
    "    tgt_vocab: !ref <output_neurons>\n",
    "    d_model: !ref <d_model>\n",
    "    nhead: !ref <nhead>\n",
    "    num_encoder_layers: !ref <num_encoder_layers>\n",
    "    num_decoder_layers: 6\n",
    "    d_ffn: !ref <d_ffn>\n",
    "    dropout: !ref <transformer_dropout>\n",
    "    activation: !ref <activation>\n",
    "    encoder_module: conformer\n",
    "    attention_type: !ref <attention_type>\n",
    "    normalize_before: True\n",
    "\n",
    "tokenizer: !new:sentencepiece.SentencePieceProcessor\n",
    "\n",
    "ctc_lin: !new:speechbrain.nnet.linear.Linear\n",
    "    input_size: !ref <d_model>\n",
    "    n_neurons: !ref <output_neurons>\n",
    "\n",
    "normalize: !new:speechbrain.processing.features.InputNormalization\n",
    "    norm_type: global\n",
    "    update_until_epoch: 4\n",
    "\n",
    "modules:\n",
    "    CNN: !ref <CNN>\n",
    "    Transformer: !ref <Transformer>\n",
    "    ctc_lin: !ref <ctc_lin>\n",
    "    normalize: !ref <normalize>\n",
    "\n",
    "model: !new:torch.nn.ModuleList\n",
    "    - [!ref <CNN>, !ref <Transformer>, !ref <ctc_lin>]\n",
    "\n",
    "# define two optimizers here for two-stage training\n",
    "Adam: !name:torch.optim.Adam\n",
    "    lr: !ref <lr_adam>\n",
    "    betas: (0.9, 0.98)\n",
    "    eps: 0.000000001\n",
    "\n",
    "log_softmax: !new:torch.nn.LogSoftmax\n",
    "    dim: -1\n",
    "\n",
    "ctc_cost: !name:speechbrain.nnet.losses.ctc_loss\n",
    "    blank_index: !ref <blank_index>\n",
    "    reduction: !ref <loss_reduction>\n",
    "\n",
    "noam_annealing: !new:speechbrain.nnet.schedulers.NoamScheduler\n",
    "    lr_initial: !ref <lr_adam>\n",
    "    n_warmup_steps: 150\n",
    "\n",
    "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
    "    limit: !ref <number_of_epochs>\n",
    "\n",
    "error_rate_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats\n",
    "\n",
    "cer_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats\n",
    "   split_tokens: True\n",
    "\n",
    "# The pretrainer allows a mapping between pretrained files and instances that\n",
    "# are declared in the yaml. E.g here, we will download the file tokenizer.ckpt\n",
    "# and it will be loaded into \"tokenizer\" which is pointing to the <pretrained_lm_tokenizer_path> defined\n",
    "# before.\n",
    "pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer\n",
    "    loadables:\n",
    "        tokenizer: !ref <tokenizer>\n",
    "    paths:\n",
    "        tokenizer: !ref <pretrained_lm_tokenizer_path>/tokenizer.ckpt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "euJMqDLSWv7Y"
   },
   "outputs": [],
   "source": [
    "def dataio_prepare(hparams):\n",
    "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
    "    It also defines the data processing pipeline through user-defined functions.\n",
    "    \"\"\"\n",
    "    # Define audio pipeline. In this case, we simply read the path contained\n",
    "    # in the variable wav with the audio reader.\n",
    "    @sb.utils.data_pipeline.takes(\"wav\")\n",
    "    @sb.utils.data_pipeline.provides(\"sig\")\n",
    "    def audio_pipeline(wav):\n",
    "        \"\"\"Load the audio signal. This is done on the CPU in the `collate_fn`.\"\"\"\n",
    "        sig = sb.dataio.dataio.read_audio(wav)\n",
    "        return sig\n",
    "\n",
    "    tokenizer = hparams[\"tokenizer\"]\n",
    "    # Define text processing pipeline. We start from the raw text and then\n",
    "    # encode it using the tokenizer. The tokens with BOS are used for feeding\n",
    "    # decoder during training, the tokens with EOS for computing the cost function.\n",
    "    # The tokens without BOS or EOS is for computing CTC loss.\n",
    "    @sb.utils.data_pipeline.takes(\"words\")\n",
    "    @sb.utils.data_pipeline.provides(\n",
    "        \"wrd\", \"tokens_list\", \"tokens_bos\", \"tokens_eos\", \"tokens\"\n",
    "    )\n",
    "    def text_pipeline(wrd):\n",
    "        \"\"\"Processes the transcriptions to generate proper labels\"\"\"\n",
    "        yield wrd\n",
    "        tokens_list = tokenizer.encode_as_ids(wrd)\n",
    "        yield tokens_list\n",
    "        tokens_bos = torch.LongTensor([hparams[\"bos_index\"]] + (tokens_list))\n",
    "        yield tokens_bos\n",
    "        tokens_eos = torch.LongTensor(tokens_list + [hparams[\"eos_index\"]])\n",
    "        yield tokens_eos\n",
    "        tokens = torch.LongTensor(tokens_list)\n",
    "        yield tokens\n",
    "\n",
    "    # Define datasets from json data manifest file\n",
    "    # Define datasets sorted by ascending lengths for efficiency\n",
    "    datasets = {}\n",
    "    data_folder = hparams[\"data_folder\"]\n",
    "    for dataset in [\"train\", \"valid\", \"test\"]:\n",
    "        datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
    "            json_path=hparams[f\"{dataset}_annotation\"],\n",
    "            replacements={\"data_root\": data_folder},\n",
    "            dynamic_items=[audio_pipeline, text_pipeline],\n",
    "            output_keys=[\n",
    "                \"id\",\n",
    "                \"sig\",\n",
    "                \"wrd\",\n",
    "                \"tokens_bos\",\n",
    "                \"tokens_eos\",\n",
    "                \"tokens\",\n",
    "            ],\n",
    "        )\n",
    "        hparams[f\"{dataset}_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    datasets[\"train\"] = datasets[\"train\"].filtered_sorted(sort_key=\"length\")\n",
    "    hparams[\"train_dataloader_opts\"][\"shuffle\"] = False\n",
    "\n",
    "    return (\n",
    "        datasets[\"train\"],\n",
    "        datasets[\"valid\"],\n",
    "        datasets[\"test\"],\n",
    "        tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "wUEBBUOQZ4V-"
   },
   "outputs": [],
   "source": [
    "# Define training procedure\n",
    "class BaseASR(sb.Brain):\n",
    "    def __init__(\n",
    "        self,\n",
    "        modules=None,\n",
    "        opt_class=None,\n",
    "        hparams=None,\n",
    "        run_opts=None,\n",
    "        checkpointer=None,\n",
    "        profiler=None,\n",
    "        tokenizer=None,\n",
    "    ):\n",
    "        super(BaseASR, self).__init__(\n",
    "            modules=modules,\n",
    "            opt_class=opt_class,\n",
    "            hparams=hparams,\n",
    "            run_opts=run_opts,\n",
    "            checkpointer=checkpointer,\n",
    "            profiler=profiler\n",
    "        )\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Performs a forward pass through the encoder\"\"\"\n",
    "        batch = batch.to(self.device)\n",
    "        wavs, wav_lens = batch.sig\n",
    "        tokens_bos, _ = batch.tokens_bos\n",
    "\n",
    "        # compute features\n",
    "        ####TASK MAKE APPROPRIATE FUNCTION CALLS TO COMPUTE THE FEATURES BELOW\n",
    "        feats = self.hparams.compute_features(wavs)\n",
    "        current_epoch = self.hparams.epoch_counter.current\n",
    "        feats = self.modules.normalize(feats, wav_lens, epoch=current_epoch)\n",
    "\n",
    "        # forward modules\n",
    "        src = self.modules.CNN(feats)\n",
    "\n",
    "        enc_out, _ = self.modules.Transformer(\n",
    "            src, tokens_bos, wav_lens, pad_idx=self.hparams.pad_index,\n",
    "        )\n",
    "\n",
    "        # output layer for ctc log-probabilities\n",
    "        logits = self.modules.ctc_lin(enc_out)\n",
    "\n",
    "        ####TASK CALCULATE THE PROBABILITIES OF THESE LOGITS\n",
    "        #### USING SPEECHBRAIN\n",
    "        p_ctc = self.hparams.log_softmax(logits)\n",
    "\n",
    "        # Compute outputs\n",
    "        hyps = None\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            hyps = None\n",
    "        else:\n",
    "            hyps = sb.decoders.ctc_greedy_decode(\n",
    "                p_ctc, wav_lens, blank_id=self.hparams.blank_index\n",
    "            )\n",
    "\n",
    "        return p_ctc, wav_lens, hyps\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"\"\"Computes the CTC loss given predictions and targets.\"\"\"\n",
    "\n",
    "        (p_ctc, wav_lens, hyps,) = predictions\n",
    "\n",
    "        ids = batch.id\n",
    "        tokens_eos, tokens_eos_lens = batch.tokens_eos\n",
    "        tokens, tokens_lens = batch.tokens\n",
    "\n",
    "        # Calculate CTC loss\n",
    "        ####TASK Make required function call to compute CTC LOSS\n",
    "        #### You have to aggregate the loss in the end so make appropriate\n",
    "        #### modifications to the value returned.\n",
    "        loss = self.hparams.ctc_cost(p_ctc, tokens, wav_lens, tokens_lens)\n",
    "\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            # Decode token terms to words\n",
    "            predicted_words = [\n",
    "                self.tokenizer.decode_ids(utt_seq).split(\" \") for utt_seq in hyps\n",
    "            ]\n",
    "            target_words = [wrd.split(\" \") for wrd in batch.wrd]\n",
    "            self.wer_metric.append(ids, predicted_words, target_words)\n",
    "            self.cer_metric.append(ids, predicted_words, target_words)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_evaluate_start(self, max_key=None, min_key=None):\n",
    "        \"\"\"Performs checkpoint averge if needed\"\"\"\n",
    "        super().on_evaluate_start()\n",
    "\n",
    "        ckpts = self.checkpointer.find_checkpoints(\n",
    "            max_key=max_key, min_key=min_key\n",
    "        )\n",
    "        ckpt = sb.utils.checkpoints.average_checkpoints(\n",
    "            ckpts, recoverable_name=\"model\", device=self.device\n",
    "        )\n",
    "\n",
    "        self.hparams.model.load_state_dict(ckpt, strict=True)\n",
    "        self.hparams.model.eval()\n",
    "        print(\"Loaded the average\")\n",
    "\n",
    "    def evaluate_batch(self, batch, stage):\n",
    "        \"\"\"Computations needed for validation/test batches\"\"\"\n",
    "        with torch.no_grad():\n",
    "            predictions = self.compute_forward(batch, stage=stage)\n",
    "            loss = self.compute_objectives(predictions, batch, stage=stage)\n",
    "        return loss.detach()\n",
    "\n",
    "    def on_stage_start(self, stage, epoch):\n",
    "        \"\"\"Gets called at the beginning of each epoch\"\"\"\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.cer_metric = self.hparams.cer_computer()\n",
    "            self.wer_metric = self.hparams.error_rate_computer()\n",
    "\n",
    "    def on_stage_end(self, stage, stage_loss, epoch):\n",
    "        \"\"\"Gets called at the end of a epoch.\"\"\"\n",
    "        # Compute/store important stats\n",
    "        stage_stats = {\"loss\": stage_loss}\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            self.train_stats = stage_stats\n",
    "        else:\n",
    "            stage_stats[\"CER\"] = self.cer_metric.summarize(\"error_rate\")\n",
    "            stage_stats[\"WER\"] = self.wer_metric.summarize(\"error_rate\")\n",
    "\n",
    "        # log stats and save checkpoint at end-of-epoch\n",
    "        if stage == sb.Stage.VALID and sb.utils.distributed.if_main_process():\n",
    "\n",
    "            lr = self.hparams.noam_annealing.current_lr\n",
    "            steps = self.optimizer_step\n",
    "            optimizer = self.optimizer.__class__.__name__\n",
    "\n",
    "            epoch_stats = {\n",
    "                \"epoch\": epoch,\n",
    "                \"lr\": lr,\n",
    "                \"steps\": steps,\n",
    "                \"optimizer\": optimizer,\n",
    "            }\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                stats_meta=epoch_stats,\n",
    "                train_stats=self.train_stats,\n",
    "                valid_stats=stage_stats,\n",
    "            )\n",
    "            # Save only last 10 checkpoints\n",
    "            self.checkpointer.save_and_keep_only(\n",
    "                meta={\"loss\": stage_loss, \"epoch\": epoch},\n",
    "                max_keys=[\"epoch\"],\n",
    "                num_to_keep=10,\n",
    "            )\n",
    "\n",
    "        elif stage == sb.Stage.TEST:\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
    "                test_stats=stage_stats,\n",
    "            )\n",
    "            # Write the WER metric for test dataset\n",
    "            if if_main_process():\n",
    "                with open(self.hparams.test_wer_file, \"w\") as w:\n",
    "                    self.wer_metric.write_stats(w)\n",
    "\n",
    "    def fit_batch(self, batch):\n",
    "        \"\"\"Performs a forward + backward pass on 1 batch\n",
    "        \"\"\"\n",
    "\n",
    "        should_step = self.step % self.grad_accumulation_factor == 0\n",
    "\n",
    "        outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
    "        loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)\n",
    "        loss.backward()\n",
    "        if self.check_gradients(loss):\n",
    "            self.optimizer.step()\n",
    "        self.zero_grad()\n",
    "        self.optimizer_step += 1\n",
    "        self.hparams.noam_annealing(self.optimizer)\n",
    "\n",
    "        self.on_fit_batch_end(batch, outputs, loss, should_step)\n",
    "        return loss.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "DDQEQ8M2MNAi"
   },
   "outputs": [],
   "source": [
    "task_hyperparameters = \"\"\"\n",
    "# Setup the directory to host experiment results\n",
    "output_folder: !ref results/transformer/Task_1\n",
    "wer_file: !ref <output_folder>/wer.txt\n",
    "save_folder: !ref <output_folder>/save\n",
    "train_log: !ref <output_folder>/train_log.txt\n",
    "\n",
    "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
    "    save_file: !ref <train_log>\n",
    "\n",
    "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
    "    checkpoints_dir: !ref <save_folder>\n",
    "    recoverables:\n",
    "        model: !ref <model>\n",
    "        noam_scheduler: !ref <noam_annealing>\n",
    "        normalizer: !ref <normalize>\n",
    "        counter: !ref <epoch_counter>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tGonNC7u8hlJ",
    "outputId": "b997e3b1-5904-4d83-e03e-63105d3d93e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: results/transformer/Task_1\n",
      "speechbrain.pretrained.fetching - Fetch tokenizer.ckpt: Using existing file/symlink in model_checkpoints/tokenizer.ckpt.\n",
      "speechbrain.utils.parameter_transfer - Set local path in self.paths[tokenizer] = model_checkpoints/tokenizer.ckpt\n",
      "speechbrain.utils.parameter_transfer - Loading pretrained files for: tokenizer\n",
      "speechbrain.utils.parameter_transfer - Redirecting (loading from local path): model_checkpoints/tokenizer.ckpt -> model_checkpoints/tokenizer.ckpt\n",
      "speechbrain.core - Info: max_grad_norm arg from hparam file is used\n",
      "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
      "speechbrain.core - 4.1M trainable parameters in BaseASR\n"
     ]
    }
   ],
   "source": [
    "hyperparams = global_hyperparams + task_hyperparameters\n",
    "hparams = load_hyperpyyaml(hyperparams)\n",
    "\n",
    "# Create experiment directory\n",
    "sb.create_experiment_directory(\n",
    "    experiment_directory=hparams[\"output_folder\"],\n",
    "    overrides=None,\n",
    ")\n",
    "\n",
    "# Here we create the datasets objects as well as tokenization and encoding\n",
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    test_data,\n",
    "    tokenizer\n",
    ") = dataio_prepare(hparams)\n",
    "\n",
    "# We download the pretrained LM from HuggingFace (or elsewhere depending on\n",
    "# the path given in the YAML file). The tokenizer is loaded at the same time.\n",
    "run_on_main(hparams[\"pretrainer\"].collect_files)\n",
    "hparams[\"pretrainer\"].load_collected(device=run_opts[\"device\"])\n",
    "\n",
    "# Trainer initialization\n",
    "asr_brain = BaseASR(\n",
    "    modules=hparams[\"modules\"],\n",
    "    opt_class=hparams[\"Adam\"],\n",
    "    hparams=hparams,\n",
    "    checkpointer=hparams[\"checkpointer\"],\n",
    "    run_opts=run_opts,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# # adding objects to trainer:\n",
    "train_dataloader_opts = hparams[\"train_dataloader_opts\"]\n",
    "valid_dataloader_opts = hparams[\"valid_dataloader_opts\"]\n",
    "\n",
    "# # Training\n",
    "# asr_brain.fit(\n",
    "#     asr_brain.hparams.epoch_counter,\n",
    "#     train_data,\n",
    "#     valid_data,\n",
    "#     train_loader_kwargs=train_dataloader_opts,\n",
    "#     valid_loader_kwargs=valid_dataloader_opts\n",
    "# )\n",
    "\n",
    "# # Testing\n",
    "# asr_brain.hparams.test_wer_file = asr_brain.hparams.wer_file\n",
    "# asr_brain.evaluate(\n",
    "#     test_data,\n",
    "#     max_key=\"epoch\",\n",
    "#     test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHIRKDrPPRGW"
   },
   "source": [
    "# Part II(A): CTC is all you need\n",
    "In this section, you will train a **6-layer Conformer** encoder with both  `CTC` and `inter-CTC` losses.\n",
    "\n",
    "> Indented block\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "SdoAx39-pSAw"
   },
   "outputs": [],
   "source": [
    "class ASR_2A(BaseASR):\n",
    "    def __init__(\n",
    "        self, *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.inter_ctc_weight = self.hparams.interctc_weight\n",
    "        self.intermediate_layers = [int(layer) for layer in self.hparams.intermediate_layers.split(',')]\n",
    "\n",
    "        # Variable to hold intermediate logits for interCTC loss calculation\n",
    "        self.inter_logits = []\n",
    "\n",
    "        # TODO: Define a helper function get_intermediate_output for the forward hook\n",
    "        def get_intermediate_output(module, input, output):\n",
    "            # TODO: Complete this function\n",
    "            self.inter_logits.append(output)\n",
    "\n",
    "\n",
    "        # TODO: Register hooks for all the intermediate encoder layers of interest.\n",
    "        # TODO: Refer to register_forward_hook (https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html)\n",
    "        # TODO: Save all the hooks in a list self.hooks that you can remove later from the module\n",
    "        self.hooks = [self.modules.Transformer.encoder.layers[i-1].register_forward_hook(get_intermediate_output) for i in self.intermediate_layers] #i-1 since 1 indexed\n",
    "\n",
    "\n",
    "\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Performs a forward pass through the encoder\"\"\"\n",
    "        batch = batch.to(self.device)\n",
    "        wavs, wav_lens = batch.sig\n",
    "        tokens_bos, _ = batch.tokens_bos\n",
    "\n",
    "        # compute features\n",
    "        feats = self.hparams.compute_features(wavs)\n",
    "        current_epoch = self.hparams.epoch_counter.current\n",
    "        feats = self.modules.normalize(feats, wav_lens, epoch=current_epoch)\n",
    "\n",
    "        # forward modules\n",
    "        src = self.modules.CNN(feats)\n",
    "\n",
    "        assert len(self.inter_logits) == 0, \"self.inter_logits should be empty as we haven't done a forward pass yet\"\n",
    "        enc_out, _ = self.modules.Transformer(\n",
    "            src, tokens_bos, wav_lens, pad_idx=self.hparams.pad_index,\n",
    "        )\n",
    "\n",
    "        # Compute final layer logit\n",
    "        logits = self.modules.ctc_lin(enc_out)\n",
    "        p_ctc = self.hparams.log_softmax(logits)\n",
    "\n",
    "        # TODO: Append all the intermediate layer logits to the following list: inter_p_ctc\n",
    "        # TODO: Go through all the layers in intermediate_layers. Note that the comma-separated list in intermediate_layers is 1-indexed.\n",
    "        # TODO: Complete code below to populate inter_p_ctc\n",
    "\n",
    "        inter_p_ctc = [self.hparams.log_softmax(self.modules.ctc_lin(temp_logits[0])) for temp_logits in self.inter_logits]\n",
    "\n",
    "        # Flush the logits saved during last forward pass.\n",
    "        self.inter_logits = []\n",
    "\n",
    "        # Compute outputs\n",
    "        hyps = None\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            assert len(inter_p_ctc) != 0 , \"inter_p_ctc should NOT be empty as forward pass is already done\"\n",
    "            hyps = None\n",
    "        else:\n",
    "            hyps = sb.decoders.ctc_greedy_decode(\n",
    "                p_ctc, wav_lens, blank_id=self.hparams.blank_index\n",
    "            )\n",
    "\n",
    "        return p_ctc, inter_p_ctc, wav_lens, hyps\n",
    "\n",
    "    def on_evaluate_start(self, max_key=None, min_key=None):\n",
    "        \"\"\"Performs sanity operations before inferencing on the test set.\"\"\"\n",
    "        if self.checkpointer is not None:\n",
    "            self.checkpointer.recover_if_possible(\n",
    "                max_key=max_key,\n",
    "                min_key=min_key,\n",
    "                device=torch.device(self.device),\n",
    "            )\n",
    "\n",
    "        # Deregister hooks here as they are not needed during evaluation\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"\"\"Computes the CTC + inter-CTC loss given predictions and targets.\"\"\"\n",
    "\n",
    "        (p_ctc, inter_p_ctc, wav_lens, hyps,) = predictions\n",
    "\n",
    "        ids = batch.id\n",
    "        tokens_eos, tokens_eos_lens = batch.tokens_eos\n",
    "        tokens, tokens_lens = batch.tokens\n",
    "\n",
    "        # TODO: Compute inter-CTC loss\n",
    "        loss_inter_ctc = sum(self.hparams.ctc_cost(temp_inter_p_ctc, tokens, wav_lens, tokens_lens) for temp_inter_p_ctc in inter_p_ctc)\n",
    "        # TODO: Write code to appropriately accumulate the inter-CTC loss in loss_inter_ctc\n",
    "        # TODO: using the softmax probabilities saved for each intermediate layer in inter_p_ctc\n",
    "\n",
    "        # Compute final layer CTC loss\n",
    "        loss_ctc = self.hparams.ctc_cost(p_ctc, tokens, wav_lens, tokens_lens)\n",
    "\n",
    "        # Compute final loss as a weighted combination of inter-CTC and CTC\n",
    "        loss = self.inter_ctc_weight * loss_inter_ctc + (1 - self.inter_ctc_weight) * loss_ctc\n",
    "\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            # Decode token terms to words\n",
    "            predicted_words = [\n",
    "                    self.tokenizer.decode_ids(utt_seq).split(\" \") for utt_seq in hyps\n",
    "                ]\n",
    "            target_words = [wrd.split(\" \") for wrd in batch.wrd]\n",
    "            self.wer_metric.append(ids, predicted_words, target_words)\n",
    "            self.cer_metric.append(ids, predicted_words, target_words)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "aoIb-4TdnyC4"
   },
   "outputs": [],
   "source": [
    "task_hyperparameters = \"\"\"\n",
    "\n",
    "# Setup the directory to host experiment results\n",
    "output_folder: !ref results/transformer/Part_2A\n",
    "wer_file: !ref <output_folder>/wer.txt\n",
    "save_folder: !ref <output_folder>/save\n",
    "train_log: !ref <output_folder>/train_log.txt\n",
    "\n",
    "interctc_weight: 0.3\n",
    "intermediate_layers: '1,2,3,4,5'\n",
    "\n",
    "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
    "    save_file: !ref <train_log>\n",
    "\n",
    "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
    "    checkpoints_dir: !ref <save_folder>\n",
    "    recoverables:\n",
    "        model: !ref <model>\n",
    "        noam_scheduler: !ref <noam_annealing>\n",
    "        normalizer: !ref <normalize>\n",
    "        counter: !ref <epoch_counter>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AX-kl1dKMx-Q",
    "outputId": "56c8da19-5e6f-4b9e-ed09-9172a778b271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: results/transformer/Part_2A\n",
      "speechbrain.pretrained.fetching - Fetch tokenizer.ckpt: Using existing file/symlink in model_checkpoints/tokenizer.ckpt.\n",
      "speechbrain.utils.parameter_transfer - Set local path in self.paths[tokenizer] = model_checkpoints/tokenizer.ckpt\n",
      "speechbrain.utils.parameter_transfer - Loading pretrained files for: tokenizer\n",
      "speechbrain.utils.parameter_transfer - Redirecting (loading from local path): model_checkpoints/tokenizer.ckpt -> model_checkpoints/tokenizer.ckpt\n",
      "speechbrain.core - Info: max_grad_norm arg from hparam file is used\n",
      "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
      "speechbrain.core - 4.1M trainable parameters in ASR_2A\n"
     ]
    }
   ],
   "source": [
    "hyperparams = global_hyperparams + task_hyperparameters\n",
    "hparams = load_hyperpyyaml(hyperparams)\n",
    "\n",
    "# # Create experiment directory\n",
    "sb.create_experiment_directory(\n",
    "    experiment_directory=hparams[\"output_folder\"],\n",
    "    overrides=None,\n",
    ")\n",
    "\n",
    "# We download the pretrained LM from HuggingFace (or elsewhere depending on\n",
    "# the path given in the YAML file). The tokenizer is loaded at the same time.\n",
    "run_on_main(hparams[\"pretrainer\"].collect_files)\n",
    "hparams[\"pretrainer\"].load_collected(device=run_opts[\"device\"])\n",
    "\n",
    "# Trainer initialization\n",
    "asr_brain = ASR_2A(\n",
    "    modules=hparams[\"modules\"],\n",
    "    opt_class=hparams[\"Adam\"],\n",
    "    hparams=hparams,\n",
    "    checkpointer=hparams[\"checkpointer\"],\n",
    "    run_opts=run_opts,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# # adding objects to trainer:\n",
    "# train_dataloader_opts = hparams[\"train_dataloader_opts\"]\n",
    "# valid_dataloader_opts = hparams[\"valid_dataloader_opts\"]\n",
    "\n",
    "# # Training\n",
    "# asr_brain.fit(\n",
    "#     asr_brain.hparams.epoch_counter,\n",
    "#     train_data,\n",
    "#     valid_data,\n",
    "#     train_loader_kwargs=train_dataloader_opts,\n",
    "#     valid_loader_kwargs=valid_dataloader_opts\n",
    "# )\n",
    "\n",
    "# # Testing\n",
    "\n",
    "# asr_brain.hparams.test_wer_file = asr_brain.hparams.wer_file\n",
    "# asr_brain.evaluate(\n",
    "#     test_data,\n",
    "#     max_key=\"ACC\",\n",
    "#     test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06q84-WOPblt"
   },
   "source": [
    "# Task 2.2: The PowerConv Module\n",
    "In this section, we will update the Conformer encoder by replacing Convolution with **PowerConv**. Rest of the architecture remains the same. Note this will be added on top of inter-CTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "BwQRTzBE3Qga"
   },
   "outputs": [],
   "source": [
    "task_hyperparameters = \"\"\"\n",
    "\n",
    "# Setup the directory to host experiment results\n",
    "output_folder: !ref results/transformer/Task_2B\n",
    "wer_file: !ref <output_folder>/wer.txt\n",
    "save_folder: !ref <output_folder>/save\n",
    "train_log: !ref <output_folder>/train_log.txt\n",
    "\n",
    "interctc_weight: 0.3\n",
    "intermediate_layers: '1,2,3,4,5'\n",
    "\n",
    "train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger\n",
    "    save_file: !ref <train_log>\n",
    "\n",
    "checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer\n",
    "    checkpoints_dir: !ref <save_folder>\n",
    "    recoverables:\n",
    "        model: !ref <model>\n",
    "        noam_scheduler: !ref <noam_annealing>\n",
    "        normalizer: !ref <normalize>\n",
    "        counter: !ref <epoch_counter>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "1gneZWFH7TmC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import speechbrain as sb\n",
    "\n",
    "from speechbrain.nnet.attention import (\n",
    "    RelPosMHAXL,\n",
    "    MultiheadAttention,\n",
    "    PositionalwiseFeedForward,\n",
    ")\n",
    "from speechbrain.nnet.normalization import LayerNorm\n",
    "from speechbrain.nnet.activations import Swish\n",
    "from speechbrain.nnet.CNN import Conv1d\n",
    "\n",
    "class PowerConv(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        kernel_size=31,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # We upsample our input by a factor of 2 to\n",
    "        input_size = input_size*2\n",
    "        self.input_size = input_size\n",
    "\n",
    "        n_channels = input_size // 2  # split input channels\n",
    "\n",
    "        # TODO: First projection feedforward layer to upsample the input\n",
    "        self.channel_proj1 = torch.nn.Linear(n_channels,input_size)\n",
    "        self.norm1 = LayerNorm(n_channels)\n",
    "        self.norm2 = LayerNorm(n_channels)\n",
    "        self.conv = Conv1d(in_channels=n_channels, out_channels=n_channels,groups=n_channels, kernel_size=kernel_size)\n",
    "        ### TODO: Use the groups parameter in the Conv1D class and set it to n_channels\n",
    "        ### TODO: Note that this conv operator does not change the feature dimensionality.\n",
    "        ### TODO: Use the appropriate value for the padding parameter in the Conv1D class to keep the feature dimensionality unaltered.\n",
    "\n",
    "        # TODO: Second projection feedforward layer\n",
    "        self.channel_proj2 = torch.nn.Linear(n_channels,n_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize convolution with ones.\n",
    "        torch.nn.init.ones_(self.conv.conv.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Shape of input x: (B, T, D)\n",
    "            Return output of shape: (B, T, D)\n",
    "        \"\"\"\n",
    "        # TODO: Implement the PowerConv module as described in the assignment pdf\n",
    "        x = self.norm1(x)\n",
    "        V = self.channel_proj1(x)\n",
    "        v1,v2 = V[...,:self.input_size//2],V[...,self.input_size//2:]\n",
    "        v2 = self.norm2(v2)\n",
    "        v2 = self.conv(v2)\n",
    "        z = v1*v2\n",
    "        x = self.dropout(z)\n",
    "        x = self.channel_proj2(x)\n",
    "        return x\n",
    "\n",
    "class CustomConformerEncoderLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        d_ffn,\n",
    "        nhead,\n",
    "        kernel_size=31,\n",
    "        kdim=None,\n",
    "        vdim=None,\n",
    "        activation=Swish,\n",
    "        bias=True,\n",
    "        dropout=0.0,\n",
    "        causal=False,\n",
    "        attention_type=\"RelPosMHAXL\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Self attention block\n",
    "        if attention_type == \"regularMHA\":\n",
    "            self.mha_layer = MultiheadAttention(\n",
    "                nhead=nhead,\n",
    "                d_model=d_model,\n",
    "                dropout=dropout,\n",
    "                kdim=kdim,\n",
    "                vdim=vdim,\n",
    "            )\n",
    "        elif attention_type == \"RelPosMHAXL\":\n",
    "            # transformerXL style positional encoding\n",
    "            self.mha_layer = RelPosMHAXL(\n",
    "                num_heads=nhead,\n",
    "                embed_dim=d_model,\n",
    "                dropout=dropout,\n",
    "                mask_pos_future=causal,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Unknown attention type\")\n",
    "\n",
    "        # Create instance of our custom convolution block\n",
    "        self.convolution_module = PowerConv(\n",
    "            d_model, kernel_size, dropout\n",
    "        )\n",
    "\n",
    "        # Feed forward macaron block\n",
    "        self.ffn_module1 = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(d_model),\n",
    "            PositionalwiseFeedForward(\n",
    "                d_ffn=d_ffn,\n",
    "                input_size=d_model,\n",
    "                dropout=dropout,\n",
    "                activation=activation,\n",
    "            ),\n",
    "            torch.nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        # Feed forward block\n",
    "        self.ffn_module2 = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(d_model),\n",
    "            PositionalwiseFeedForward(\n",
    "                d_ffn=d_ffn,\n",
    "                input_size=d_model,\n",
    "                dropout=dropout,\n",
    "                activation=activation,\n",
    "            ),\n",
    "            torch.nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.drop = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        src_mask = None,\n",
    "        src_key_padding_mask = None,\n",
    "        pos_embs = None,\n",
    "    ):\n",
    "        conv_mask = None\n",
    "        if src_key_padding_mask is not None:\n",
    "            conv_mask = src_key_padding_mask.unsqueeze(-1)\n",
    "\n",
    "        # ffn module\n",
    "        x = x + 0.5 * self.ffn_module1(x)\n",
    "\n",
    "        # muti-head attention module\n",
    "        skip = x\n",
    "        x = self.norm1(x)\n",
    "        x, self_attn = self.mha_layer(\n",
    "            x,\n",
    "            x,\n",
    "            x,\n",
    "            attn_mask=src_mask,\n",
    "            key_padding_mask=src_key_padding_mask,\n",
    "            pos_embs=pos_embs,\n",
    "        )\n",
    "        x = x + skip\n",
    "\n",
    "        # convolution module\n",
    "        x = x + self.convolution_module(x)\n",
    "\n",
    "        # ffn module\n",
    "        x = self.norm2(x + 0.5 * self.ffn_module2(x))\n",
    "\n",
    "        return x, self_attn\n",
    "\n",
    "\n",
    "class CustomConformerEncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        d_model,\n",
    "        d_ffn,\n",
    "        nhead,\n",
    "        kernel_size=31,\n",
    "        kdim=None,\n",
    "        vdim=None,\n",
    "        activation=Swish,\n",
    "        bias=True,\n",
    "        dropout=0.0,\n",
    "        causal=False,\n",
    "        attention_type=\"RelPosMHAXL\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create layers using our custom encoder layer that utilizes PowerConv\n",
    "        self.layers = torch.nn.ModuleList(\n",
    "            [\n",
    "                CustomConformerEncoderLayer(\n",
    "                    d_ffn=d_ffn,\n",
    "                    nhead=nhead,\n",
    "                    d_model=d_model,\n",
    "                    kdim=kdim,\n",
    "                    vdim=vdim,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation,\n",
    "                    kernel_size=kernel_size,\n",
    "                    bias=bias,\n",
    "                    causal=causal,\n",
    "                    attention_type=attention_type,\n",
    "                )\n",
    "                for i in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = LayerNorm(d_model, eps=1e-6)\n",
    "        self.attention_type = attention_type\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        src_mask = None,\n",
    "        src_key_padding_mask = None,\n",
    "        pos_embs = None,\n",
    "    ):\n",
    "\n",
    "        if self.attention_type == \"RelPosMHAXL\":\n",
    "            if pos_embs is None:\n",
    "                raise ValueError(\n",
    "                    \"The chosen attention type for the Conformer is RelPosMHAXL. For this attention type, the positional embeddings are mandatory\"\n",
    "                )\n",
    "\n",
    "        output = src\n",
    "        attention_lst = []\n",
    "        # Loop through the encoder layers\n",
    "        for enc_layer in self.layers:\n",
    "            output, attention = enc_layer(\n",
    "                output,\n",
    "                src_mask=src_mask,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                pos_embs=pos_embs,\n",
    "            )\n",
    "            attention_lst.append(attention)\n",
    "        output = self.norm(output)\n",
    "\n",
    "        return output, attention_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "3wgsf3f27VuR"
   },
   "outputs": [],
   "source": [
    "class ASR_2B(ASR_2A):\n",
    "    def __init__(\n",
    "        self, device=\"cpu\", *args, **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # Remove the old hooks as they are not useful\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "        # Instantiate our custom encoder that uses PowerConv\n",
    "        encoder = CustomConformerEncoder(\n",
    "            nhead=self.hparams.nhead,\n",
    "            num_layers=self.hparams.num_encoder_layers,\n",
    "            d_ffn=self.hparams.d_ffn,\n",
    "            d_model=self.hparams.d_model,\n",
    "            dropout=self.hparams.transformer_dropout,\n",
    "            activation=self.hparams.activation,\n",
    "            attention_type=self.hparams.attention_type,\n",
    "        ).to(device)\n",
    "\n",
    "        # Replace the standard encoder with our encoder\n",
    "        self.modules.Transformer.encoder = encoder\n",
    "\n",
    "        self.inter_logits = []\n",
    "        def get_intermediate_output(module, input, output):\n",
    "          self.inter_logits.append(output)\n",
    "\n",
    "        self.hooks = [self.modules.Transformer.encoder.layers[i-1].register_forward_hook(get_intermediate_output) for i in self.intermediate_layers] #i-1 since 1 indexed\n",
    "        # TODO: Copy this code from your implemention in Part II(A) within the __init__ function of ASR_2A that populates self.hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1pcomwa-zwj",
    "outputId": "41e8f004-c795-4cdc-81f4-53c02470ba96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.core - Beginning experiment!\n",
      "speechbrain.core - Experiment folder: results/transformer/Task_2B\n",
      "speechbrain.pretrained.fetching - Fetch tokenizer.ckpt: Using existing file/symlink in model_checkpoints/tokenizer.ckpt.\n",
      "speechbrain.utils.parameter_transfer - Set local path in self.paths[tokenizer] = model_checkpoints/tokenizer.ckpt\n",
      "speechbrain.utils.parameter_transfer - Loading pretrained files for: tokenizer\n",
      "speechbrain.utils.parameter_transfer - Redirecting (loading from local path): model_checkpoints/tokenizer.ckpt -> model_checkpoints/tokenizer.ckpt\n",
      "speechbrain.core - Info: max_grad_norm arg from hparam file is used\n",
      "speechbrain.core - Info: ckpt_interval_minutes arg from hparam file is used\n",
      "speechbrain.core - 4.1M trainable parameters in ASR_2B\n",
      "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n",
      "speechbrain.utils.epoch_loop - Going into epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:42<00:00,  1.85it/s, train_loss=966]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:08<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 1, lr: 8.91e-03, steps: 190, optimizer: Adam - train loss: 9.66e+02 - valid loss: 5.14e+02, valid CER: 1.00e+02, valid WER: 1.00e+02\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-36-00+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.epoch_loop - Going into epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:27<00:00,  2.18it/s, train_loss=951]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:59<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 2, lr: 6.29e-03, steps: 380, optimizer: Adam - train loss: 9.51e+02 - valid loss: 5.02e+02, valid CER: 1.00e+02, valid WER: 1.00e+02\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-38-27+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.epoch_loop - Going into epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:37<00:00,  1.95it/s, train_loss=898]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:05<00:00,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 3, lr: 5.13e-03, steps: 570, optimizer: Adam - train loss: 8.98e+02 - valid loss: 4.62e+02, valid CER: 1.00e+02, valid WER: 1.00e+02\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-41-10+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.epoch_loop - Going into epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:42<00:00,  1.85it/s, train_loss=789]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:01<00:00,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 4, lr: 4.45e-03, steps: 760, optimizer: Adam - train loss: 7.89e+02 - valid loss: 3.88e+02, valid CER: 76.25, valid WER: 95.74\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-43-55+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.epoch_loop - Going into epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:40<00:00,  1.89it/s, train_loss=693]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:08<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 5, lr: 3.98e-03, steps: 950, optimizer: Adam - train loss: 6.93e+02 - valid loss: 3.57e+02, valid CER: 63.17, valid WER: 91.74\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-46-44+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.epoch_loop - Going into epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:40<00:00,  1.89it/s, train_loss=647]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:36<00:00,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 6, lr: 3.63e-03, steps: 1140, optimizer: Adam - train loss: 6.47e+02 - valid loss: 3.40e+02, valid CER: 61.78, valid WER: 91.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-49-02+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:24<00:00,  7.84it/s, train_loss=613]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:11<00:00, 11.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 7, lr: 3.36e-03, steps: 1330, optimizer: Adam - train loss: 6.13e+02 - valid loss: 3.25e+02, valid CER: 55.97, valid WER: 88.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-49-39+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:25<00:00,  7.38it/s, train_loss=586]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:13<00:00, 10.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 8, lr: 3.14e-03, steps: 1520, optimizer: Adam - train loss: 5.86e+02 - valid loss: 3.17e+02, valid CER: 53.30, valid WER: 87.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-50-18+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:23<00:00,  8.03it/s, train_loss=562]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:13<00:00, 10.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 9, lr: 2.96e-03, steps: 1710, optimizer: Adam - train loss: 5.62e+02 - valid loss: 3.08e+02, valid CER: 50.80, valid WER: 86.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-50-56+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:24<00:00,  7.80it/s, train_loss=543]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:11<00:00, 11.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 10, lr: 2.81e-03, steps: 1900, optimizer: Adam - train loss: 5.43e+02 - valid loss: 2.99e+02, valid CER: 49.89, valid WER: 85.72\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-51-33+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.epoch_loop - Going into epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:25<00:00,  7.40it/s, train_loss=525]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:18<00:00,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 11, lr: 2.68e-03, steps: 2090, optimizer: Adam - train loss: 5.25e+02 - valid loss: 2.93e+02, valid CER: 47.91, valid WER: 84.76\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-52-18+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-36-00+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:28<00:00,  6.60it/s, train_loss=509]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:12<00:00, 10.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 12, lr: 2.57e-03, steps: 2280, optimizer: Adam - train loss: 5.09e+02 - valid loss: 2.88e+02, valid CER: 46.68, valid WER: 83.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-53-01+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-38-27+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:35<00:00,  5.36it/s, train_loss=495]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:20<00:00,  6.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 13, lr: 2.46e-03, steps: 2470, optimizer: Adam - train loss: 4.95e+02 - valid loss: 2.83e+02, valid CER: 44.98, valid WER: 82.85\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-53-58+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-41-10+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:27<00:00,  6.89it/s, train_loss=481]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:15<00:00,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 14, lr: 2.38e-03, steps: 2660, optimizer: Adam - train loss: 4.81e+02 - valid loss: 2.79e+02, valid CER: 43.16, valid WER: 81.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-54-43+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-43-55+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:30<00:00,  6.23it/s, train_loss=469]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:15<00:00,  9.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 15, lr: 2.29e-03, steps: 2850, optimizer: Adam - train loss: 4.69e+02 - valid loss: 2.76e+02, valid CER: 41.98, valid WER: 81.39\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-55-30+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-46-44+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:33<00:00,  5.67it/s, train_loss=457]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:14<00:00,  9.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 16, lr: 2.22e-03, steps: 3040, optimizer: Adam - train loss: 4.57e+02 - valid loss: 2.73e+02, valid CER: 41.32, valid WER: 80.75\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-56-19+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-49-02+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:28<00:00,  6.68it/s, train_loss=446]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:15<00:00,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 17, lr: 2.16e-03, steps: 3230, optimizer: Adam - train loss: 4.46e+02 - valid loss: 2.71e+02, valid CER: 40.76, valid WER: 79.49\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-57-05+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-49-39+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:29<00:00,  6.40it/s, train_loss=437]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:12<00:00, 10.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 18, lr: 2.09e-03, steps: 3420, optimizer: Adam - train loss: 4.37e+02 - valid loss: 2.66e+02, valid CER: 39.07, valid WER: 78.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-57-48+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-50-18+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:25<00:00,  7.40it/s, train_loss=428]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:18<00:00,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 19, lr: 2.04e-03, steps: 3610, optimizer: Adam - train loss: 4.28e+02 - valid loss: 2.67e+02, valid CER: 39.04, valid WER: 78.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-58-34+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-50-56+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:24<00:00,  7.69it/s, train_loss=419]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:17<00:00,  7.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 20, lr: 1.99e-03, steps: 3800, optimizer: Adam - train loss: 4.19e+02 - valid loss: 2.66e+02, valid CER: 38.46, valid WER: 77.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-59-18+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-51-33+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:29<00:00,  6.48it/s, train_loss=411]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:20<00:00,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 21, lr: 1.94e-03, steps: 3990, optimizer: Adam - train loss: 4.11e+02 - valid loss: 2.62e+02, valid CER: 37.89, valid WER: 76.87\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-00-09+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-52-18+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:27<00:00,  7.00it/s, train_loss=403]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:16<00:00,  8.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 22, lr: 1.89e-03, steps: 4180, optimizer: Adam - train loss: 4.03e+02 - valid loss: 2.62e+02, valid CER: 37.30, valid WER: 76.59\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-00-54+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-53-01+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:29<00:00,  6.52it/s, train_loss=396]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:15<00:00,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 23, lr: 1.85e-03, steps: 4370, optimizer: Adam - train loss: 3.96e+02 - valid loss: 2.58e+02, valid CER: 36.45, valid WER: 76.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-01-40+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-53-58+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:32<00:00,  5.80it/s, train_loss=389]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:15<00:00,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 24, lr: 1.81e-03, steps: 4560, optimizer: Adam - train loss: 3.89e+02 - valid loss: 2.57e+02, valid CER: 36.47, valid WER: 75.78\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-02-30+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-54-43+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:24<00:00,  7.61it/s, train_loss=382]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:22<00:00,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 25, lr: 1.78e-03, steps: 4750, optimizer: Adam - train loss: 3.82e+02 - valid loss: 2.56e+02, valid CER: 36.05, valid WER: 75.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-03-19+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-55-30+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:30<00:00,  6.30it/s, train_loss=376]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:15<00:00,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 26, lr: 1.74e-03, steps: 4940, optimizer: Adam - train loss: 3.76e+02 - valid loss: 2.54e+02, valid CER: 35.52, valid WER: 75.09\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-04-06+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-56-19+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:29<00:00,  6.51it/s, train_loss=368]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:19<00:00,  6.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 27, lr: 1.71e-03, steps: 5130, optimizer: Adam - train loss: 3.68e+02 - valid loss: 2.53e+02, valid CER: 34.91, valid WER: 74.49\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-04-56+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-57-05+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:34<00:00,  5.45it/s, train_loss=364]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:20<00:00,  6.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 28, lr: 1.68e-03, steps: 5320, optimizer: Adam - train loss: 3.64e+02 - valid loss: 2.53e+02, valid CER: 34.57, valid WER: 74.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-05-54+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-57-48+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:26<00:00,  7.29it/s, train_loss=357]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:14<00:00,  9.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 29, lr: 1.65e-03, steps: 5510, optimizer: Adam - train loss: 3.57e+02 - valid loss: 2.51e+02, valid CER: 34.20, valid WER: 73.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-06-35+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-58-34+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:29<00:00,  6.41it/s, train_loss=351]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:19<00:00,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 30, lr: 1.62e-03, steps: 5700, optimizer: Adam - train loss: 3.51e+02 - valid loss: 2.51e+02, valid CER: 33.99, valid WER: 73.45\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-07-26+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+22-59-18+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:31<00:00,  6.01it/s, train_loss=346]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:15<00:00,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 31, lr: 1.60e-03, steps: 5890, optimizer: Adam - train loss: 3.46e+02 - valid loss: 2.51e+02, valid CER: 33.95, valid WER: 72.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-08-15+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-00-09+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:26<00:00,  7.12it/s, train_loss=343]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:21<00:00,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 32, lr: 1.57e-03, steps: 6080, optimizer: Adam - train loss: 3.43e+02 - valid loss: 2.50e+02, valid CER: 33.54, valid WER: 72.20\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-09-04+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-00-54+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:29<00:00,  2.13it/s, train_loss=337]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:05<00:00,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 33, lr: 1.55e-03, steps: 6270, optimizer: Adam - train loss: 3.37e+02 - valid loss: 2.50e+02, valid CER: 33.39, valid WER: 71.67\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-11-41+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-01-40+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:35<00:00,  1.98it/s, train_loss=333]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:56<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 34, lr: 1.52e-03, steps: 6460, optimizer: Adam - train loss: 3.33e+02 - valid loss: 2.49e+02, valid CER: 33.24, valid WER: 71.33\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-14-15+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-02-30+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:30<00:00,  2.10it/s, train_loss=328]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:10<00:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 35, lr: 1.50e-03, steps: 6650, optimizer: Adam - train loss: 3.28e+02 - valid loss: 2.49e+02, valid CER: 33.40, valid WER: 70.91\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-16-59+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-03-19+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:52<00:00,  1.69it/s, train_loss=323]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:18<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 36, lr: 1.48e-03, steps: 6840, optimizer: Adam - train loss: 3.23e+02 - valid loss: 2.49e+02, valid CER: 33.00, valid WER: 71.10\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-20-11+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-04-06+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:45<00:00,  1.80it/s, train_loss=320]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:11<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 37, lr: 1.46e-03, steps: 7030, optimizer: Adam - train loss: 3.20e+02 - valid loss: 2.47e+02, valid CER: 32.44, valid WER: 70.19\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-23-10+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-04-56+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:45<00:00,  1.80it/s, train_loss=315]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:12<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 38, lr: 1.44e-03, steps: 7220, optimizer: Adam - train loss: 3.15e+02 - valid loss: 2.48e+02, valid CER: 32.55, valid WER: 69.92\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-26-09+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-05-54+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:43<00:00,  1.83it/s, train_loss=312]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:08<00:00,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 39, lr: 1.42e-03, steps: 7410, optimizer: Adam - train loss: 3.12e+02 - valid loss: 2.47e+02, valid CER: 32.03, valid WER: 69.52\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-29-03+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-06-35+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:28<00:00,  2.16it/s, train_loss=308]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:07<00:00,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 40, lr: 1.40e-03, steps: 7600, optimizer: Adam - train loss: 3.08e+02 - valid loss: 2.46e+02, valid CER: 31.79, valid WER: 69.40\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-31-41+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-07-26+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:36<00:00,  1.97it/s, train_loss=304]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:59<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 41, lr: 1.39e-03, steps: 7790, optimizer: Adam - train loss: 3.04e+02 - valid loss: 2.48e+02, valid CER: 31.96, valid WER: 69.32\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-34-18+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-08-15+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:40<00:00,  1.90it/s, train_loss=299]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:04<00:00,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 42, lr: 1.37e-03, steps: 7980, optimizer: Adam - train loss: 2.99e+02 - valid loss: 2.46e+02, valid CER: 31.62, valid WER: 68.97\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-37-04+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-09-04+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:38<00:00,  1.94it/s, train_loss=297]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:02<00:00,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 43, lr: 1.36e-03, steps: 8170, optimizer: Adam - train loss: 2.97e+02 - valid loss: 2.47e+02, valid CER: 31.61, valid WER: 69.24\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-39-46+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-11-41+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:38<00:00,  1.93it/s, train_loss=293]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:59<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 44, lr: 1.34e-03, steps: 8360, optimizer: Adam - train loss: 2.93e+02 - valid loss: 2.44e+02, valid CER: 31.36, valid WER: 68.95\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-42-26+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-14-15+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:37<00:00,  1.95it/s, train_loss=290]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:06<00:00,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 45, lr: 1.32e-03, steps: 8550, optimizer: Adam - train loss: 2.90e+02 - valid loss: 2.48e+02, valid CER: 31.72, valid WER: 68.78\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-45-11+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-16-59+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:36<00:00,  1.96it/s, train_loss=286]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:52<00:00,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 46, lr: 1.31e-03, steps: 8740, optimizer: Adam - train loss: 2.86e+02 - valid loss: 2.46e+02, valid CER: 31.16, valid WER: 68.23\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-47-42+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-20-11+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:37<00:00,  1.95it/s, train_loss=284]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:00<00:00,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 47, lr: 1.30e-03, steps: 8930, optimizer: Adam - train loss: 2.84e+02 - valid loss: 2.47e+02, valid CER: 30.97, valid WER: 67.56\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-50-22+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-23-10+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:39<00:00,  1.92it/s, train_loss=281]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:00<00:00,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 48, lr: 1.28e-03, steps: 9120, optimizer: Adam - train loss: 2.81e+02 - valid loss: 2.46e+02, valid CER: 30.86, valid WER: 67.70\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-53-03+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-26-09+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:27<00:00,  2.17it/s, train_loss=278]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:56<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 49, lr: 1.27e-03, steps: 9310, optimizer: Adam - train loss: 2.78e+02 - valid loss: 2.45e+02, valid CER: 30.59, valid WER: 67.63\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-55-29+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-29-03+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:32<00:00,  2.06it/s, train_loss=275]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:59<00:00,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 50, lr: 1.26e-03, steps: 9500, optimizer: Adam - train loss: 2.75e+02 - valid loss: 2.48e+02, valid CER: 30.67, valid WER: 66.57\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-58-03+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-31-41+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:26<00:00,  2.20it/s, train_loss=273]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:01<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 51, lr: 1.24e-03, steps: 9690, optimizer: Adam - train loss: 2.73e+02 - valid loss: 2.44e+02, valid CER: 30.38, valid WER: 66.69\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-00-33+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-34-18+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:26<00:00,  2.20it/s, train_loss=269]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:58<00:00,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 52, lr: 1.23e-03, steps: 9880, optimizer: Adam - train loss: 2.69e+02 - valid loss: 2.48e+02, valid CER: 30.76, valid WER: 67.55\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-03-00+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-37-04+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:25<00:00,  2.22it/s, train_loss=267]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:00<00:00,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 53, lr: 1.22e-03, steps: 10070, optimizer: Adam - train loss: 2.67e+02 - valid loss: 2.45e+02, valid CER: 30.32, valid WER: 67.13\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-05-28+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-39-46+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:24<00:00,  2.25it/s, train_loss=264]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:58<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 54, lr: 1.21e-03, steps: 10260, optimizer: Adam - train loss: 2.64e+02 - valid loss: 2.45e+02, valid CER: 30.20, valid WER: 66.67\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-07-53+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-42-26+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:27<00:00,  2.17it/s, train_loss=262]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:05<00:00,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 55, lr: 1.20e-03, steps: 10450, optimizer: Adam - train loss: 2.62e+02 - valid loss: 2.46e+02, valid CER: 29.96, valid WER: 67.08\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-10-28+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-45-11+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [02:40<00:00,  1.18it/s, train_loss=259]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:49<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 56, lr: 1.19e-03, steps: 10640, optimizer: Adam - train loss: 2.59e+02 - valid loss: 2.45e+02, valid CER: 29.80, valid WER: 65.83\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-15-00+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-47-42+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:15<00:00,  2.52it/s, train_loss=256]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:05<00:00,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 57, lr: 1.18e-03, steps: 10830, optimizer: Adam - train loss: 2.56e+02 - valid loss: 2.49e+02, valid CER: 30.39, valid WER: 66.71\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-17-23+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-50-22+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:24<00:00,  2.25it/s, train_loss=254]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:59<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 58, lr: 1.17e-03, steps: 11020, optimizer: Adam - train loss: 2.54e+02 - valid loss: 2.45e+02, valid CER: 30.02, valid WER: 66.84\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-19-49+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-53-03+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:25<00:00,  2.23it/s, train_loss=252]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:58<00:00,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 59, lr: 1.16e-03, steps: 11210, optimizer: Adam - train loss: 2.52e+02 - valid loss: 2.47e+02, valid CER: 30.01, valid WER: 66.02\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-22-15+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-55-29+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:29<00:00,  2.12it/s, train_loss=250]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:08<00:00,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 60, lr: 1.15e-03, steps: 11400, optimizer: Adam - train loss: 2.50e+02 - valid loss: 2.47e+02, valid CER: 29.69, valid WER: 65.77\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-24-55+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-18+23-58-03+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:30<00:00,  2.10it/s, train_loss=248]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [01:09<00:00,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 61, lr: 1.14e-03, steps: 11590, optimizer: Adam - train loss: 2.48e+02 - valid loss: 2.47e+02, valid CER: 30.22, valid WER: 67.16\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-27-36+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-00-33+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [01:34<00:00,  2.01it/s, train_loss=245]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:20<00:00,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 62, lr: 1.13e-03, steps: 11780, optimizer: Adam - train loss: 2.45e+02 - valid loss: 2.47e+02, valid CER: 29.44, valid WER: 65.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-29-33+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-03-00+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:24<00:00,  7.78it/s, train_loss=244]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:15<00:00,  8.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 63, lr: 1.12e-03, steps: 11970, optimizer: Adam - train loss: 2.44e+02 - valid loss: 2.47e+02, valid CER: 29.76, valid WER: 66.03\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-30-15+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-05-28+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:26<00:00,  7.10it/s, train_loss=242]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:13<00:00, 10.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 64, lr: 1.11e-03, steps: 12160, optimizer: Adam - train loss: 2.42e+02 - valid loss: 2.48e+02, valid CER: 29.55, valid WER: 66.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-30-56+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-07-53+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:23<00:00,  8.17it/s, train_loss=240]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:15<00:00,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 65, lr: 1.10e-03, steps: 12350, optimizer: Adam - train loss: 2.40e+02 - valid loss: 2.45e+02, valid CER: 29.21, valid WER: 65.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-31-37+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-10-28+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:20<00:00,  9.16it/s, train_loss=237]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:13<00:00,  9.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 66, lr: 1.09e-03, steps: 12540, optimizer: Adam - train loss: 2.37e+02 - valid loss: 2.48e+02, valid CER: 29.65, valid WER: 65.83\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-32-14+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-15-00+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:27<00:00,  6.93it/s, train_loss=235]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:21<00:00,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 67, lr: 1.09e-03, steps: 12730, optimizer: Adam - train loss: 2.35e+02 - valid loss: 2.46e+02, valid CER: 29.33, valid WER: 65.18\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-33-05+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-17-23+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:24<00:00,  7.91it/s, train_loss=234]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:15<00:00,  9.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 68, lr: 1.08e-03, steps: 12920, optimizer: Adam - train loss: 2.34e+02 - valid loss: 2.48e+02, valid CER: 29.52, valid WER: 65.58\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-33-45+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-19-49+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:33<00:00,  5.60it/s, train_loss=230]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:13<00:00, 10.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 69, lr: 1.07e-03, steps: 13110, optimizer: Adam - train loss: 2.30e+02 - valid loss: 2.48e+02, valid CER: 29.11, valid WER: 65.21\n",
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-34-35+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-22-15+00\n",
      "speechbrain.utils.epoch_loop - Going into epoch 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 190/190 [00:24<00:00,  7.87it/s, train_loss=230]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 137/137 [00:19<00:00,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - epoch: 70, lr: 1.06e-03, steps: 13300, optimizer: Adam - train loss: 2.30e+02 - valid loss: 2.47e+02, valid CER: 29.29, valid WER: 65.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.checkpoints - Saved an end-of-epoch checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-35-21+00\n",
      "speechbrain.utils.checkpoints - Deleted checkpoint in results/transformer/Task_2B/save/CKPT+2024-02-19+00-24-55+00\n",
      "speechbrain.utils.checkpoints - Would load a checkpoint here, but none found yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 328/328 [00:45<00:00,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speechbrain.utils.train_logger - Epoch loaded: 70 - test loss: 85.28, test CER: 29.59, test WER: 66.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85.2838506407854"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparams = global_hyperparams + task_hyperparameters\n",
    "hparams = load_hyperpyyaml(hyperparams)\n",
    "\n",
    "# Create experiment directory\n",
    "sb.create_experiment_directory(\n",
    "    experiment_directory=hparams[\"output_folder\"],\n",
    "    overrides=None,\n",
    ")\n",
    "\n",
    "# We download the pretrained LM from HuggingFace (or elsewhere depending on\n",
    "# the path given in the YAML file). The tokenizer is loaded at the same time.\n",
    "run_on_main(hparams[\"pretrainer\"].collect_files)\n",
    "hparams[\"pretrainer\"].load_collected(device=run_opts[\"device\"])\n",
    "\n",
    "# Trainer initialization\n",
    "asr_brain = ASR_2B(\n",
    "    modules=hparams[\"modules\"],\n",
    "    opt_class=hparams[\"Adam\"],\n",
    "    hparams=hparams,\n",
    "    checkpointer=hparams[\"checkpointer\"],\n",
    "    run_opts=run_opts,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# adding objects to trainer:\n",
    "train_dataloader_opts = hparams[\"train_dataloader_opts\"]\n",
    "valid_dataloader_opts = hparams[\"valid_dataloader_opts\"]\n",
    "\n",
    "# Training\n",
    "asr_brain.fit(\n",
    "    asr_brain.hparams.epoch_counter,\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_loader_kwargs=train_dataloader_opts,\n",
    "    valid_loader_kwargs=valid_dataloader_opts\n",
    ")\n",
    "\n",
    "# Testing\n",
    "\n",
    "asr_brain.hparams.test_wer_file = asr_brain.hparams.wer_file\n",
    "asr_brain.evaluate(\n",
    "    test_data,\n",
    "    max_key=\"ACC\",\n",
    "    test_loader_kwargs=hparams[\"test_dataloader_opts\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "bKXIYTJSOnt_",
    "rtdr1VnyCQTJ"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
